# talk-to-your-code-rag
Talk to Your Code â€“ RAG-Based GitHub Repository Assistant ğŸ”ğŸ“‚
ğŸ“Œ Overview

This project implements a Retrieval-Augmented Generation (RAG) system that allows users to interact with a GitHub code repository using natural language queries.

Instead of manually searching through files, users can ask questions like:

â€œCode for decision treesâ€

â€œWhere is Random Forest implemented?â€

â€œGradient Boosting related filesâ€

The system retrieves the most relevant source code from the repository using semantic search and presents it as the answer context.

ğŸ¯ Objective

The goal of this project is to demonstrate how RAG can be applied to source code repositories by combining:

Code ingestion from GitHub

Semantic embeddings

Vector databases

Retrieval-based question answering

This project focuses on grounded answers, ensuring responses are based strictly on retrieved repository code.

ğŸ§  What is RAG?

Retrieval-Augmented Generation (RAG) is a technique that enhances language models by grounding their responses in retrieved documents.

In this project:

Retrieval is performed using vector similarity search

Generation (optional) can be added using an LLM (e.g., OpenAI)

The system avoids hallucinations by relying on real source code

ğŸ—ï¸ Architecture
GitHub Repository
        â†“
Load Python Files (.py)
        â†“
Split Code into Chunks
        â†“
Generate Embeddings
        â†“
Store in Vector Database (Chroma)
        â†“
Semantic Retrieval
        â†“
Answer User Queries

âš™ï¸ Technologies Used

Python

LangChain

ChromaDB (Vector Database)

Cohere / OpenAI Embeddings

GitPython

Google Colab

ğŸ“‚ Project Workflow
Step 1: Clone GitHub Repository

The repository is cloned locally using GitPython.

Step 2: Load Code Files

Only .py files are loaded for processing.

Step 3: Chunking

Large code files are split into smaller chunks to improve retrieval accuracy.

Step 4: Embeddings & Vector Database

Code chunks are converted into vector embeddings.

Embeddings are stored in ChromaDB for fast semantic search.

Step 5: Retrieval-Based Q&A

User queries are converted into embeddings.

Relevant code snippets are retrieved based on similarity.

Retrieved code is returned as the answer context.

âš ï¸ Note: LLM-based generation can be integrated optionally depending on API availability.

ğŸ§ª Example Queries
Code for decision trees
Code for Random Forest
Code for Gradient Boosting

ğŸ“Œ Why No LLM Output

Due to recent API deprecations and access restrictions in some LLM providers, this project demonstrates retrieval-grounded answers by returning relevant source code directly.

This still fulfills the core objective of RAG, where answers are grounded in retrieved documents rather than free-form generation.

ğŸš€ Future Enhancements

Add OpenAI-based generation for natural language answers

Support multiple programming languages

Add a web-based UI (Streamlit / FastAPI)

Implement multi-query and reranking techniques

Enable repository-wide summarization

ğŸ“„ Conclusion

This project demonstrates a practical and real-world use of Retrieval-Augmented Generation for understanding and navigating large codebases. It highlights how semantic search and vector databases can significantly improve developer productivity.

ğŸ‘¤ Author
Rakshith Vellulla
Masterâ€™s in Computer Science
RAG / NLP / Machine Learning Enthusiast
